```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, results = FALSE, cache = TRUE)
```

```{r libraries}
library(here) # Load here
library(readr)
library(parallel)
library(runjags)
library(dplyr) # used for arrange(), to rank hospitals
library(ggplot2)
library(gridExtra)
library(mcmcplots) # for running mean plot
library(ggmcmc)
```

```{r globals}
theme_set(theme_bw())
data_path <- "data/BurnoutPA.txt"
burn_in_iterations <- 4000
sample_iterations <- 10000
method <- "rjags"
n_chains <- 5

# Convenience function for plotting
plot_jags <- function(model, type, var) {
    plot(model, plot.type = type, vars = var, family = "UGent Panno Text")
}
```

```{r load}
RN4CAST_complete <-
    read_table( # read_table has as default skip_empty_rows = TRUE, so only rows without missing data are read in
        here(data_path),
        col_names = TRUE,
        col_types = cols(
            col_integer(),
            col_integer(),
            col_factor(),
            col_factor(),
            col_integer(),
            col_factor(),
            col_double(),
            col_double(),
            col_factor(),
            col_integer()
        )
    )
```

```{r scale}
RN4CAST_complete$beds <- c(scale(RN4CAST_complete$beds))
RN4CAST_complete$we <- c(scale(RN4CAST_complete$we))
RN4CAST_complete$expe <- c(scale(RN4CAST_complete$expe))
```

# Answers

**Part 1: Although the variable PA does not correspond to count data, a Poisson model will be
fitted for the purpose of this exercise.**

**Fit a three-level Poisson random intercept model, with nursing unit representing the level 2
random intercept and hospital representing the level 3 random intercept. Assume normal
distributions for the random intercepts.**

- **Take vague priors for all model parameters. Determine whether an inverse gamma or a uniform prior is most appropriate for the variability parameter of the random intercepts.**

The random intercepts model we fit here partitions the total variance in variance between hospitals $\epsilon_{h}\sim N(0,\sigma_h^2)$, variance between units within a hospital $\epsilon_{h,u}\sim N(0,\sigma_{h,u}^2)$, and residual variance $\epsilon_{h,u,n}$. The priors we need to decide on are for $\sigma_h^2$ and $\sigma_{h,u}^2$, the hospital and unit random intercept variance, respectively. The debate among Bayesians on the inverse gamma and uniform priors for variance parameters in hierarchical models seems unsettled: there is an argument to be made for both in theory [@10.1214/06-BA117A]. We also note that our data set is not small and the likelihood should therefore dominate the prior, as long as the latter is not absurd.

- **First, fit a model without the covariates and check convergence using classical diagnostics.**

As we are asked to model the dependent variable personal accomplishment $Y$ as a Poisson random variable using only random intercepts, we use the exponential link function to obtain the following equation:

$$\log(Y)=\beta_0+\epsilon_{h}+\epsilon_{h,u}+\epsilon_{h,u,n}$$

Of course, we need translate this equation and the associated priors into code for an MCMC sampler.
We perform our analysis in JAGS [@hornik2003jags] using the `runjags` package [@JSSv071i09] for the R statistical programming language.
This package allows us to both generate code for JAGS and instruct JAGS to execute it.
However, while it supports setting a gamma precision prior (or inverse gamma variance prior) out of the box, it does not support setting an inverse uniform precision prior (or uniform variance prior) out of the box. In that case we write the JAGS code ourselves.

Before we cover the prior parameters and performance of both models, let us give an idea of general settings we choose for JAGS.
Since these two initial random effects models are arguably complex, we opt to instruct JAGS to sample from 5 Markov chains.
We do the same for all following models.
When we sample from these chains in parallel, trying both the `parallel` and `rjparallel` options to the `runjags::run.jags` function, we find their implementation lacking: we encounter segmentation faults and have to resample from the chains to calculate DIC.
Due to these issues and the fact that the execution time with the sequential implementation of `runjags` is not dramatically worse, we abandon the parallel implementation.
Finally, we choose an initial burn-in period of 4,000 and a sample period of 10,000.
Should convergence prove difficult, then we extend the chains with additional samples.

Let us first assess the performance of the model with a $U(0.001,1000)$ prior on the variance parameters.
We set this prior's lower range parameter to $0.001$ to avoid divisions by $0$.
We make this prior wide because we are not domain experts: it is difficult for us to assess the contribution of hospitals and units to the reduced personal accomplishment dimension of burnout.
This is of course a theme throughout our work here, so we do not dwell on it later in this report.

```{r}
JAGS.mod_uniform <- run.jags(
    "JAGSmodel0_unif.txt",
    data = RN4CAST_complete,
    monitor = c("intercept", "unit_precision", "hosp_precision", "deviance", "hosp_randomeffect"),
    burnin = burn_in_iterations,
    sample = sample_iterations,
    method = "rjags",
    n.chains = n_chains
)
```

Before assessing the model performance through formal measures, we opt for a visual inspection. 
Figure \@ref(fig:modUniformConvergence) depicts trace, autocorrelation, and running mean plots for the precision esimates of the random intercepts for unit (`unit_precision`) and for hospital (`hosp_precision`).
These serve to assess the ergodicity of the chains.
The autocorrelation seems limited: for `unit_precision`, the autocorrelation is almost 0 at lag 15. For `hosp_precision`, the descent is less pronounced: it reaches 0 around lag 25. The trace plot for `unit_precision` shows the chains are in agreement and the running mean plot indicates a mean which quickly becomes stable. We note similar plots for `hosp_precision`.

```{r modUniformConvergence, fig.ncol = 3, out.width = "33%", fig.show='hold', fig.align = "center", fig.cap="Trace ((a) and (d)), autocorrelation ((b) and (e)), and running mean ((c) and (f)) plots for the model with uniform priors on random intercept variances, without covariates", fig.subcap=c('', '', '', '')}
par(mfrow = c(3, 2))
plot_jags(JAGS.mod_uniform, "trace", "unit_precision")
plot_jags(JAGS.mod_uniform, "autocorr", "unit_precision")
rmeanplot(JAGS.mod_uniform, parms = "unit_precision")
plot_jags(JAGS.mod_uniform, "trace", "hosp_precision")
plot_jags(JAGS.mod_uniform, "autocorr", "hosp_precision")
rmeanplot(JAGS.mod_uniform, parms = "hosp_precision")
```

```{r, results = TRUE}
JAGS.mod_uniform
```

We now proceed to the formal measures we obtain from `runjags` and show above.
The Gelman-Rubin ANOVA diagnostic or estimated potential scale
reduction factor (PSRF) is close to 1 for both the `unit_precision` and
the `hosp_precision`, which is the aim. The MCMC error is also low as a percentage of comparison to the standard deviation: 1.1 % for `unit_precision` and 2 % for `hosp_precision`.
We also obtain the deviance information criterion (DIC) below to be able to compare the goodness-of-fit of this model to the model with gamma priors on precision later.

```{r, results = TRUE}
dic_uniform <- extract.runjags(JAGS.mod_uniform, what = "dic")
dic_uniform
```

```{r, results = TRUE}
sum(dic_uniform$deviance) + sum(dic_uniform$penalty)
```

Let us now turn our attention to the model with the gamma prior on precision.
We implement this model as the JAGS code in `JAGSmodel0_inversegamma.txt`, included with this report.
We keep the default scale and shape parameters, both $0.001$, for the gamma prior on the precision of the two random intercepts. As for the model with uniform priors on variance, we use the default vague normal prior $N(0,10^{-6})$ for the fixed intercept.

```{r}
model0_gamma <- template.jags(
    formula = pa ~ (1 | unit) + (1 | hosp),
    data = RN4CAST_complete,
    file = "src/JAGSmodel0_inversegamma.txt",
    precision.prior = "dgamma(0.001, 0.001)",
    family = "poisson",
    write.data = FALSE,
    n.chains = 5
)
```
  
```{r}
JAGS.mod_gamma <- run.jags(
  model0_gamma,
  data = RN4CAST_complete,
  monitor = c("intercept", "unit_precision", "hosp_precision", "full_effect", "deviance"),
  burnin = burn_in_iterations,
  sample = sample_iterations,
  method = "rjags",
  n.chains = 5
)
```

Before assessing the model performance through formal measures, we first do a visual inspection as we did for the previous model.
Figure \@ref(fig:inverseGammaConvergence) depicts the trace, autocorrelation, and running mean plots for the precision of the random intercept for unit (`unit_precision`) and hospital (`hosp_precision`).
  
```{r inverseGammaConvergence, fig.ncol = 3, out.width = "33%", fig.show='hold', fig.align = "center", fig.cap="race ((a) and (d)), autocorrelation ((b) and (e)), and running mean ((c) and (f)) plots for the model with inverse gamma priors on random intercept variances, without covariates", fig.subcap=c('', '', '', '')}
par(mfrow = c(3, 2))
plot(JAGS.mod_gamma, plot.type = "trace", vars = "unit_precision")
plot(JAGS.mod_gamma, plot.type = "autocorr", vars = "unit_precision")
rmeanplot(JAGS.mod_gamma, parms = "unit_precision")
plot(JAGS.mod_gamma, plot.type = "trace", vars = "hosp_precision")
plot(JAGS.mod_gamma, plot.type = "autocorr", vars = "hosp_precision")
rmeanplot(JAGS.mod_gamma, parms = "hosp_precision")
```

The autocorrelation plots show that the autocorrelation is limited: about 0 at lag 15 for `unit_precision` and about 0 at lag 25 for `hosp_precision`.
We conclude the autocorrelation is similar to the autocorrelation for the model with uniform variance priors.
The trace plots for `unit_precision` and `hosp_precision` indicate agreement between the chains.
The running mean plots are as stable as for the previous model.
What we do see is that this model does not agree with the previous model's estimate of `hosp_precision`: it is notably higher for this model.

```{r poisson-gamma-summary, results = TRUE}
JAGS.mod_gamma
```

The PSRF is again close to 1, for both `unit_precision` and `hosp_precision`, which is what we aim for. The MCMC error is also small as a percentage of the standard deviation: 0.9 % for `unit_precision` and 1.4 % for `hosp_precision`.
While we see that the 95 % credible interval (CI) for `unit_precision` overlaps with the same CI for the previous model, the 95 % CI for `hosp_precision` is much wider. To compare the goodness-of-fit of this model to that of the previous model, we obtain the DIC below.

```{r _gamma, results=TRUE}
dic_gamma <- extract.runjags(JAGS.mod_gamma, what = "dic")
dic_gamma
```

This DIC is only slightly worse than that obtained under uniform priors for the variance. Such limited effect was to be expected given the large amount of data in the dataset.
Given this fact and its more concentrated posterior for `hosp_precision`, we decide to continue with the model with uniform priors for the variance.

- **Rank the hospitals according to their random effect (in WinBUGS it is possible with the rank option). Are there hospitals with important differences in the PA baseline level?**

``` {r ranked, results = TRUE, fig.ncol = 2, out.width = "100%", fig.show='hold', fig.align = "center"}
hosp_randomeffects <- add.summary(JAGS.mod_uniform, vars = "hosp_randomeffect") # obtain hosp_randomeffects out 
hosp_randomeffects_means <-  data.frame(print(hosp_randomeffects))["Mean"] # transform runjags.object to dataframe to enable further handling
hosp_randomeffects_ranked <- arrange(hosp_randomeffects_means, hosp_randomeffects_means$Mean) # sort intercepts

print(hosp_randomeffects_ranked)
histogram <- ggplot(hosp_randomeffects_ranked, aes(x = Mean)) + geom_histogram()
mcmc.uniform_sample<-as.mcmc(JAGS.mod_uniform)
mcmc.sample<-ggs(mcmc.uniform_sample)
caterpillar <- ggs_caterpillar(mcmc.sample,family=c("hosp_randomeffect"))
grid.arrange(histogram, caterpillar, ncol = 2, nrow = 1)
```

There seem to be hospitals with noticeable differences in the PA baseline level.

- **In a second step, include the covariates. Are there hospitals genuinely different with respect to the PA level after adjusting for the covariates in the model?**

The JAGS code for this model is in `JAGSmodel_unif_cov.txt`.
  
```{r}
JAGS.mod_uniform_cov <- run.jags(
  # specify the syntax file
  "JAGSmodel_unif_cov.txt",
  # specify the data source (only necessary when write.data = FALSE)
  data = RN4CAST_complete,
  monitor = c(
    "intercept", "unit_precision", "hosp_precision",
    "expe_coefficient", "full_effect", "unitsur_effect",
    "we_coefficient", "tech_effect", "teach_effect", "beds_coefficient",
    "deviance", "dic", "hosp_randomeffect"
  ),
  burnin = burn_in_iterations, # make informed decision later (choosing speed here)
  sample = sample_iterations, # make informed decision later (choosing speed here)
  method = "rjags",
  n.chains = n_chains
)
```

Before assessing the model based through formal measures, we first do a visual inspection as done before.
Figure \@ref(fig:uniformFullTrace) shows trace plots for the precision of the random intercepts and effects of the covariates and figure \@ref(fig:uniformFullAuto) shows their autocorrelation plots.
All 9 trace plots indicate agreement among the chains.
Autocorrelation does not seem to be an issue: it generally reaches 0 at lag 10 to lag 15, but the autocorrelation for `unit_precision` reaches 0 at lag 25. 
The running means plots in figure \@ref(fig:uniformFullMean) depict stable means.
  
```{r uniformFullTrace, fig.ncol = 3, out.width="33%", fig.show='hold', fig.align = "center", fig.cap="Trace plots for the model with uniform priors on random intercept variances, using all covariates", fig.subcap=c('', '', '', ''), fig.align='center'}
par(mfrow = c(3, 3))
plot_jags(JAGS.mod_uniform_cov, "trace", "unit_precision")
plot_jags(JAGS.mod_uniform_cov, "trace", "hosp_precision")
plot_jags(JAGS.mod_uniform_cov, "trace", "expe_coefficient")
plot_jags(JAGS.mod_uniform_cov, "trace", "full_effect")
plot_jags(JAGS.mod_uniform_cov, "trace", "unitsur_effect")
plot_jags(JAGS.mod_uniform_cov, "trace", "we_coefficient")
plot_jags(JAGS.mod_uniform_cov, "trace", "tech_effect")
plot_jags(JAGS.mod_uniform_cov, "trace", "teach_effect")
plot_jags(JAGS.mod_uniform_cov, "trace", "beds_coefficient")
```

```{r uniformFullAuto, fig.ncol = 3, out.width="33%", fig.show='hold', fig.align = "center", fig.cap="Autocorrelation plots for the model with uniform priors on random intercept variances, using all covariates", fig.subcap=c('', '', '', ''), fig.align='center'}
par(mfrow = c(3, 3))
plot_jags(JAGS.mod_uniform_cov, "autocorr", "unit_precision")
plot_jags(JAGS.mod_uniform_cov, "autocorr", "hosp_precision")
plot_jags(JAGS.mod_uniform_cov, "autocorr", "expe_coefficient")
plot_jags(JAGS.mod_uniform_cov, "autocorr", "full_effect")
plot_jags(JAGS.mod_uniform_cov, "autocorr", "unitsur_effect")
plot_jags(JAGS.mod_uniform_cov, "autocorr", "we_coefficient")
plot_jags(JAGS.mod_uniform_cov, "autocorr", "tech_effect")
plot_jags(JAGS.mod_uniform_cov, "autocorr", "teach_effect")
plot_jags(JAGS.mod_uniform_cov, "autocorr", "beds_coefficient")
```

```{r uniformFullMean, fig.ncol = 3, out.width="33%", fig.show='hold', fig.align = "center", fig.cap="Running mean plots for the model with uniform priors on random intercept variances, using all covariates", fig.subcap=c('', '', '', ''), fig.align='center'}
par(mfrow = c(3, 3))
rmeanplot(JAGS.mod_uniform_cov, parms = "expe_coefficient")
rmeanplot(JAGS.mod_uniform_cov, parms = "full_effect")
rmeanplot(JAGS.mod_uniform_cov, parms = "unitsur_effect")
rmeanplot(JAGS.mod_uniform_cov, parms = "we_coefficient")
rmeanplot(JAGS.mod_uniform_cov, parms = "tech_effect")
rmeanplot(JAGS.mod_uniform_cov, parms = "teach_effect")
rmeanplot(JAGS.mod_uniform_cov, parms = "beds_coefficient")
```

```{r poisson-uniform-summary, results = TRUE}
JAGS.mod_uniform_cov
```

The PSRF is again close to 1 for all parameters.
The MCMC error is always small as a percentage of standard deviation, ranging from 0.5 to 1.7 %.

``` {r rankedCov, fig.cap = "Hospitals ranked by their random effect for the model including covariates"}
hosp_randomeffects_cov <- add.summary(JAGS.mod_uniform_cov, vars = "hosp_randomeffect") # obtain hosp_randomeffects out 
hosp_randomeffects_means_cov <-  data.frame(print(hosp_randomeffects_cov))["Mean"] # transform runjags.object to dataframe to enable further handling
hosp_randomeffects_ranked_cov <- arrange(hosp_randomeffects_means_cov, hosp_randomeffects_means_cov$Mean) # sort intercepts

print(hosp_randomeffects_ranked_cov)
histogram_cov <- ggplot(hosp_randomeffects_ranked_cov, aes(x = Mean)) + geom_histogram()
mcmc.uniform_sample_cov<-as.mcmc(JAGS.mod_uniform_cov)
mcmc.sample_cov<-ggs(mcmc.uniform_sample_cov)
caterpillar_cov <- ggs_caterpillar(mcmc.sample_cov,family=c("hosp_randomeffect"))
grid.arrange(histogram_cov, caterpillar_cov, ncol = 2, nrow = 1)
```

Figure \@ref(fig:rankedCov) shows the hospital ranking does change when controlling for covariates.
Most of the top hospitals stay near the top, but some, such as 17, do move considerably in the ranking.

- **Check with DIC whether the inclusion of the covariates improves the fit with respect to the baseline model. Determine what variables to include using this selection criterion.**

```{r, results = TRUE}
dic_uniform_cov <- extract.runjags(JAGS.mod_uniform_cov, what = "dic")
dic_uniform_cov
```

The DIC hardly changes when we naively include all covariates.
We propose forward variable selection driven by the DIC instead: starting from the model with only the random intercepts, we add each covariate in turn, find the covariate which minimizes DIC and add it to the model if the minimized DIC is lower than the previous model's DIC.
We continue this process until the DIC does not decrease further or there are no covariates left to include.
In the first round, we find that `we` is worth including, but it only decreases DIC a bit, as we see in the `runjags` output below.

```{r}
JAGS.mod_uniform_we <- run.jags(
    "src/JAGSmodel_unif_we.txt",
    data = RN4CAST_complete,
    monitor = c("intercept", "unit_precision", "hosp_precision", "deviance", "hosp_randomeffect", "we_coefficient"),
    burnin = burn_in_iterations,
    sample = sample_iterations,
    method = "rjags",
    n.chains = n_chains
)
```

```{r, results = TRUE}
(uniform_we_dic <- extract.runjags(JAGS.mod_uniform_we, what = "dic"))
```

```{r, results = TRUE}
sum(uniform_we_dic$deviance) + sum(uniform_we_dic$penalty)
```

In the second round, `unitsur` is the best candidate covariate: it minimizes DIC the most.
However, the DIC of the model including `unitsur` is still higher, due to the additional penalty we incur.

```{r}
JAGS.mod_uniform_we_unitsur <- run.jags(
    "src/JAGSmodel_unif_we_unitsur.txt",
    data = RN4CAST_complete,
    monitor = c("intercept", "unit_precision", "hosp_precision", "deviance", "hosp_randomeffect", "we_coefficient", "unitsur_effect"),
    burnin = burn_in_iterations,
    sample = sample_iterations,
    method = "rjags",
    n.chains = n_chains
)
```

```{r, results = TRUE}
(uniform_we_unitsur_dic <- extract.runjags(JAGS.mod_uniform_we_unitsur, what = "dic"))
```

```{r, results = TRUE}
sum(uniform_we_unitsur_dic$deviance) + sum(uniform_we_unitsur_dic$penalty)
```

We therefore decide to only include `we` as a covariate in our model.

```{r}
# formula <- "pa ~ (1 | unit) + (1 | hosp)"
# level_covariates <- list(c("expe", "full"), c("unitsur", "we"), c("tech", "teach", "beds"))
# used_covariates <- vector(mode = "character")
# better_dic_found <- TRUE
# monitor <- c("k", "intercept", "unit_precision", "hosp_precision")
# method <- "parallel"
# 
# JAGS   <- template.jags(
#     formula = formula,
#     data = RN4CAST_complete,
#     file = "JAGSmodel_unif.txt",
#     precision.prior = "dunif(0.001, 1000)",
#     family = "poisson",
#     write.data = FALSE,
#     n.chains = n_chains
# )
# initial_run <- run.jags(
#     initial_model,
#     data = RN4CAST_complete,
#     burnin = burn_in_iterations,
#     sample = sample_iterations,
#     method = method,
#     monitor = monitor
# )
# 
# initial_dic <- extract.runjags(initial_run, what = "dic")
# lowest_cost_so_far <- sum(initial_dic$deviance) + sum(initial_dic$penalty)
# runs <- list()
# dics <- list()
# 
# for (level in seq_len(length(level_covariates))) {
#     unused_covariates <- level_covariates[[level]]
#     while (length(unused_covariates) > 0 && better_dic_found) {
#         better_dic_found <- FALSE
#         costs <- vector(mode = "numeric", length = length(unused_covariates))
#         for (index in seq_len(length(unused_covariates))) {
#             candidate_formula <- as.formula(paste(formula, unused_covariates[index], sep = " + "))
#             print(paste("Now trying", toString(candidate_formula)))
#             candidate_model_file_name <- paste(paste("JAGSmodel", "unif", paste(used_covariates, sep = "", collapse = "_"), unused_covariates[index], sep = "_"), ".txt", sep = "")
#             candidate_model <- template.jags(
#                 formula = candidate_formula,
#                 data = RN4CAST_complete,
#                 file = candidate_model_file_name,
#                 precision.prior = "dunif(0.001, 1000)",
#                 family = "poisson",
#                 write.data = FALSE,
#                 n.chains = n_chains
#             )
#             candidate_run <- run.jags(
#                 candidate_model,
#                 data = RN4CAST_complete,
#                 burnin = burn_in_iterations,
#                 sample = sample_iterations,
#                 method = method,
#                 monitor = monitor
#             )
#             candidate_dic <- extract.runjags(candidate_run, what = "dic")
#             runs <- c(runs, candidate_run)
#             dics <- c(dics, candidate_dic)
#             costs[index] <- sum(candidate_dic$deviance) + sum(candidate_dic$penalty)
#         }
#         lowest_cost_index <- which.min(costs)
#         lowest_cost <- costs[lowest_cost_index]
#         if (lowest_cost < lowest_cost_so_far) {
#             formula <- paste(formula, unused_covariates[lowest_cost_index], sep = " + ")
#             print(paste("New minimal DIC for", toString(formula), "=", lowest_cost))
#             used_covariates <- append(used_covariates, unused_covariates[lowest_cost_index])
#             unused_covariates <- unused_covariates[-c(lowest_cost_index)]
#             lowest_cost_so_far <- lowest_cost
#             better_dic_found <- TRUE
#         }
#     }
#     better_dic_found <- TRUE
# }
```

- **Give all necessary posterior summary measures of the relevant parameters.**

```{r, results = TRUE}
JAGS.mod_uniform_we
```

- **Hint: Standardize all numeric covariates to improve convergence of the MCMC procedure and give initial values to the regression coefficients and parameters related to variances of random effects.**

We standardize all numeric covariates and give initial values to all parameters.

**Part 2: In some cases, there is overdispersion compared to what we would be expect in a Poisson model.**

- **Check whether a negative binomial distribution for PA is better than a Poisson distribution.**

The JAGS code for the negative binomial model including the `we` covariate is in `JAGSmodel_binom_uniform_cov.txt`.
  
```{r}
JAGS.mod_binom_uniform_cov <- run.jags(
  # specify the syntax file
  "src/JAGSmodel_binom_uniform_we.txt",
  # specify the data source (only necessary when write.data = FALSE)
  data = RN4CAST_complete,
  monitor = c(
    "k", "intercept", "unit_precision", "hosp_precision", "we_coefficient", "deviance"
  ),
  burnin = burn_in_iterations, # make informed decision later (choosing speed here)
  sample = sample_iterations, # make informed decision later (choosing speed here)
  method = "rjags"
)
```

Before assessing the model based through formal measures, we first do a
visual inspection. 

For example, here are the trace plots for the precision of the random
intercept for unit (`unit_precision`) and for hospital (`hosp_precision`), 
as well as for the covariates: 

The trace plots seem to mix well. Autocorrelation seems to remain high for a long time. However, give our large burn-in of 4000 iterations, this autocorrelation does not pose a problem.

```{r negativeBinomDiag, fig.ncol = 2, out.width="50%", fig.align='center', fig.show='hold', fig.align = "center", fig.cap="Trace ((a) and (d)), autocorrelation ((b) and (e)) plots for the negative binomial model", fig.subcap=c('', '', '', '')}
par(mfrow = c(2, 3))
plot_jags(JAGS.mod_binom_uniform_cov, "trace", "unit_precision")
plot_jags(JAGS.mod_binom_uniform_cov, "autocorr", "unit_precision")
plot_jags(JAGS.mod_binom_uniform_cov, "trace", "hosp_precision")
plot_jags(JAGS.mod_binom_uniform_cov, "autocorr", "hosp_precision")
plot_jags(JAGS.mod_binom_uniform_cov, "trace", "we_coefficient")
plot_jags(JAGS.mod_binom_uniform_cov, "autocorr", "hosp_precision")
```

```{r negative-binomial-covariates-summary, results = TRUE}
JAGS.mod_binom_uniform_cov
```

The PSRF is close to 1 for all parameters, which is the aim. Also, the MCMC error is relatively small as a percentage of standard deviation, ranging from 0.5 to 4.6. This is substantially higher than for the previous models, although the MCMC error remains below the 5% threshold.
We subsequently obtain the DIC to assess the model performance.
  
The results are the following:
  
```{r, results = TRUE}
dic_binom_uniform_cov <- extract.runjags(JAGS.mod_binom_uniform_cov, what = "dic")
dic_binom_uniform_cov
```

The DIC is substantially better than under the previous model, which assumes a Poisson distribution for the reduced personal accomplishment dimension of burnout.
However, we do incur a considerable penalty for switching models: it is brought about by the additional parameter the negative binomial distribution requires.

- **Motivate your conclusion.**

We conclude that the negative binomial distribution fits the data better.
We find that switching priors or including covariates makes little sense when we choose the wrong model to begin with, as evidenced by the considerable drop in DIC.
In Bayesian inference, just as in frequentist inference, the domain expertise which drives the choice of model is vital.
The difference is that in Bayesian inference, this expertise can also drive the choice of prior.
Informative priors could possibly further improve our model.

[1]: https://bmcnurs.biomedcentral.com/articles/10.1186/1472-6955-10-6 "Sermeus et al. (2011)."
