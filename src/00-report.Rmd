```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, results = FALSE, cache = TRUE)
```

```{r libraries}
library(here) # Load here
library(readr)
library(parallel)
library(runjags)
library(dplyr) # used for arrange(), to rank hospitals
library(mcmcplots) # for running mean plot
```

```{r globals}
data_path <- "data/BurnoutPA.txt"
burn_in_iterations <- 4000
sample_iterations <- 10000
more_sample_iterations <- 30000
method <- "rjags"
max_chains <- 5
n_chains <- max_chains#min(detectCores(), max_chains)

# Convenience function for plotting
plot_jags <- function(type, var) {
    plot(JAGS.mod_uniform, plot.type = type, vars = var)
    dev.off()
}
```

```{r load}
RN4CAST_complete <- (RN4CAST <-
    read_table( # read_table has as default skip_empty_rows = TRUE, so only rows without missing data are read in
        here(data_path),
        col_names = TRUE,
        col_types = cols(
            col_integer(),
            col_integer(),
            col_factor(),
            col_factor(),
            col_integer(),
            col_factor(),
            col_double(),
            col_double(),
            col_factor(),
            col_integer()
        )
    ))
```

```{r scale}
RN4CAST_complete$beds <- c(scale(RN4CAST_complete$beds))
RN4CAST_complete$we <- c(scale(RN4CAST_complete$we))
RN4CAST_complete$expe <- c(scale(RN4CAST_complete$expe))
```

# Questions

## Part 1


Introduction as added by Leah, what procedure to follow , ...

2 models next to each other  

Q1 : Determine whether an inverse gamma or a
uniform prior is most appropriate for the variability parameter of the random intercepts.

**Although the variable PA does not correspond to count data, a Poisson model will be
fitted for the purpose of this exercise.**

**Fit a three-level Poisson random intercept model, with nursing unit representing the level 2
random intercept and hospital representing the level 3 random intercept. Assume normal
distributions for the random intercepts.**

- **Take vague priors for all model parameters. Determine whether an inverse gamma or a uniform prior is most appropriate for the variability parameter of the random intercepts.**

The random intercepts model we fit here partitions the total variance in variance between hospitals $\epsilon_{h}\sim N(0,\sigma_h^2)$, variance between units within a hospital $\epsilon_{h,u}\sim N(0,\sigma_{h,u}^2)$, and residual variance $\epsilon_{h,u,n}$.

TO DO = answer the question about most 'appropiate' precision prior. (my opionion, this is also a theoretical thing : is Inverse Gamma kind of "conjugate?

TO DO : read spiegelhalter Paper on ... why inverse gamma make sense, but not using

- **First, fit a model without the covariates and check convergence using classical diagnostics.**

As we are asked to model the dependent variable personal accomplishment $Y$ as a Poisson random variable, we use the exponential link function to obtain the following equation:

$$\log(Y)=\beta_0+\epsilon_{h}+\epsilon_{h,u}+\epsilon_{h,u,n}$$
### Model without covariates: uniform prior for precision

The model is written down in a file, called `JAGSmodel0_gamma.txt`, in our
current directory. The default sets a $\text{Gamma}(0.001, .001)$ prior
on the precision of the two random intercepts and a vague
$\text{Normal}(0,10^{-6})$ prior on the fixed effects (in this case, a
fixed intercept only). Another default is the number of chains (2)..

In the first part of the assignment, we're asked to run exactly this
model, but instead we have to test what prior to put on the precision of
the random intercepts: an inverse gamma or a uniform.

#### Pilot model

##### Creating model

First, let's asses performance with a a $\text{Uniform}(0.001,100)$
prior, for which the model. Then, we run the following chunk to get the syntax for a model
with only the intercepts, a user-specified uniform prior for the
precision, and 5 chains. The amount of chains is based on the generally
accepted heuristic that 3-5 chains are needed for more complex models:

We can now just run the model directly from `R` without needing to
manually copy/paste anything. For this, we use the `run.jags` function
to run the model we just specified our syntax for. We choose a limited
burn-in (4000) and sample (10000) to allow for an initial pilot run.
<!--- Should method=rjags be  used in original model design (see documentation extend.jags - then no need to recompile model when extending) --->

```{r}
JAGS.mod_uniform <- run.jags(
    "JAGSmodel0_unif.txt",
    data = RN4CAST_complete,
    monitor = c("intercept", "unit_precision", "hosp_precision", "deviance",
    "hosp_randomeffect"), # monitor additionally all intercepts of random effect of hospitals
    burnin = burn_in_iterations,
    sample = sample_iterations,
    method = "rjags",
    n.chains = n_chains
)
```

##### Visualization

Before assessing the model based through formal measures, we first do a
visual inspection. Various plot types are available:
`plot.type = c("trace", "ecdf", "histogram", "autocorr", "crosscorr")`.
You can also specify for which variable you want the plot. The different
options are
`vars = c("intercept", "unit_precision", "hosp_precision", "deviance", "resid.sum.sq")`.

For example, here are the trace plots and autocorrelation plots for the precision of the random
intercept for unit (`unit_precision`) and for hospital (`hosp_precision`):

```{r, out.width="50%", fig.align='center'}
plot_jags("trace", "unit_precision")
plot_jags("autocorr", "unit_precision")
plot_jags("trace", "hosp_precision")
plot_jags("autocorr", "hosp_precision")
rmeanplot(JAGS.mod_uniform, parms = "unit_precision")
```

The autocorrelation seems limited: for `unit_precision`, the
autocorrelation is almost zero at lag 15. For `hosp_precision`, this is
even better. The trace plot for `unit_precision` looks fairly well, although
there is limited dependency (indicated by the "zigzagging"). Yet, the running
mean plot seems to indicate a constant mean. 
For `hosp_precision`, the trace plot is similar although there seems to 
be an upper cap depending on the range of the prior (i.c. 100).

##### Assessment model

The model ran, and we can inspect its results.

```{r, results = TRUE}
JAGS.mod_uniform
```
Since the visual inspection yields stable results, we proceed to more formal measures.
The Gelman-Rubin ANOVA diagnostic (i.e. estimated potential scale
reduction factor - PSRF) is close to 1 for both the `unit_precision` and
the `hosp_precision`, which is the aim. The MCMC error is also limited
in comparison with the SD (1.0% for `unit_precision` and 0.9%
for `hosp_precision`). 

We subsequently obtain the DIC (Deviance Information Criterion) to
assess the model performance.

```{r, results = TRUE}
dic_uniform <- extract.runjags(JAGS.mod_uniform, what = "dic")
dic_uniform
```

- **Rank the hospitals according to their random effect (in WinBUGS it is possible with the rank option). Are there hospitals with important differences in the PA baseline level?**

``` {r ranked}
hosp_randomeffects <- add.summary(JAGS.mod_uniform, vars = "hosp_randomeffect") # obtain hosp_randomeffects out 
hosp_randomeffects_means <-  data.frame(print(hosp_randomeffects))["Mean"] # transform runjags.object to dataframe to enable further handling
hosp_randomeffects_ranked <- arrange(hosp_randomeffects_means, hosp_randomeffects_means$Mean) # sort intercepts

print(hosp_randomeffects_ranked)
hist(hosp_randomeffects_ranked$Mean)
```
```{r}
library(ggmcmc)
# convert the model as mcmc for convinience of extracting plots and ranking
mcmc.uniform_sample<-as.mcmc(JAGS.mod_uniform)
mcmc.sample<-ggs(mcmc.uniform_sample)

# ranking the hospitals 
jpeg("catapillar.jpeg",quality=100)
ggs_caterpillar(mcmc.sample,family=c("hosp_randomeffect"))

```

### Model without covariates: gamma prior for precision
We now assess whether an inverse gamma prior would be a better prior.

#### Pilot model

##### Creating model

We manually adapt the previous model to use an inverse gamma prior for the precision instead of a uniform prior ("JAGSmodel0_inversegamma.txt"). We apply this manual process, since `runjags` does not allow to directly specify an inverse gamma prior using `template.jags`.

We again use the `run.jags` function to run the model we just specified our syntax for. We choose a limited burn-in (4000) and sample (10000) to allow for an initial pilot run. 
<!--- Should method=rjags be  used in original model design (see documentation extend.jags - then no need to recompile model when extending) --->

(modify the source file if needed)

```{r, message=FALSE, warning=FALSE}
model0_gamma <- template.jags(
    formula = pa ~ (1 | unit) + (1 | hosp),
    data = RN4CAST_complete,
    file = "JAGSmodel0_gamma.txt",
    precision.prior = "dgamma(0.001, 0.001)",
    family = "poisson",
    write.data = FALSE,
    n.chains = 5
)
```
  
```{r cache=TRUE}
JAGS.mod_gamma <- run.jags(
  model0_gamma,
  data = RN4CAST_complete, # specify the data source (only necessary when write.data = FALSE)
  monitor = c("intercept", "unit_precision", "hosp_precision", "full_effect", "deviance"),
  burnin = burn_in_iterations, # make informed decision later (choosing speed here)
  sample = sample_iterations, # make informed decision later (choosing speed here)
  method = "rjags",
  n.chains = 5
)
```

##### Visualization

Before assessing the model based through formal measures, we first do a visual inspection as we did for the previous model.
For example, here are the trace plots and autocorrelation plotss for the precision of the random intercept for unit (`unit_precision`) and for hospital (`hosp_precision`):
  
```{r, message=FALSE, out.width="50%", fig.align='center'}
plot(JAGS.mod_gamma, plot.type = "trace", vars = "unit_precision")
plot(JAGS.mod_gamma, plot.type = "autocorr", vars = "unit_precision")
plot(JAGS.mod_gamma, plot.type = "trace", vars = "hosp_precision")
plot(JAGS.mod_gamma, plot.type = "autocorr", vars = "hosp_precision")
# rmeanplot(JAGS.mod_uniform, parms = "unit_precision")
```
The autocorrelation plots show that the autocorrelation is limited (about 0 at lag5 for both `unit_precision` and `hosp_precision`). 
The trace plots for `unit_precision` and for `hosp_precision` look well. 

##### Assessment model

The model ran, and we can inspect its results.

```{r poisson-gamma-summary, results = TRUE}
summary(JAGS.mod_gamma)
```

The Gelman-Rubin ANOVA diagnostic (i.e. estimated potential scale reduction factor - PSRF) is close to 1 for both `unit_precision` and
 `hosp_precision`, which is the aim. Also, the MCMC error is small in comparison with the SD (0.6% for both `unit_precision` and `hosp_precision`).

We subsequently obtain the DIC (Deviance Information Criterion) to assess the model performance.

```{r _gamma}
dic_gamma <- extract.runjags(JAGS.mod_gamma, what = "dic")
dic_gamma
```

The results are the following:
  
- Mean deviance:  8892 
- penalty 122.9 
- Penalized deviance: 9015 

These results are worse than those obtained under uniform priors for the precision, although this is mainly due to the higher penalty term (given that the IG prior is a more complex modelled prior than the uniform prior used earlier).

##### Visualization

Perform visual inspection of trace plots for `unit_precision` and `hosp_precision` for extended model.
<!--- using reference of previous code block ?? --->
```{r-extend, ref.label='traceplots'}
```

```{r, message=FALSE, out.width="50%", fig.align='center'}
plot(JAGS.mod_gamma, plot.type = "trace", vars = "unit_precision")
plot(JAGS.mod_gamma, plot.type = "autocorr", vars = "unit_precision")
plot(JAGS.mod_gamma, plot.type = "trace", vars = "hosp_precision")
plot(JAGS.mod_gamma, plot.type = "autocorr", vars = "hosp_precision")
```

The trace plots for `unit_precision` and `hosp_precision` did not change a lot.

##### Assessment model

Then we inspect the results again.
<!--- using reference of previous code block ?? --->
```{r-extend, ref.label='summary-mod'}
```

```{r}
summary(JAGS.mod_gamma)
```

The Gelman-Rubin ANOVA diagnostic (i.e. estimated potential scale reduction factor - PSRF) is still close to 1 for both `unit_precision` and `hosp_precision`, which is the aim. The MCMC error has also dropped substantially in comparison with the SD (0.5 for both `unit_precision` and for `hosp_precision`). 

We subsequently obtain the DIC (Deviance Information Criterion) to assess the model performance.

```{r_gamma}
dic_gamma <- extract.runjags(JAGS.mod_gamma, what = "dic")
dic_gamma
```

The results are the following:
-   Mean deviance:  8886 
-   penalty 124.4 
-   Penalized deviance: 9010 

These are very similar to the results obtained for the shorter chains with the gamma priors. 

- **In a second step, include the covariates. Are there hospitals genuinely different with respect to the PA level after adjusting for the covariates in the model?**

### Model with covariates: uniform prior for precision

The model is written down in a file, called `JAGSmodel_unif_cov.txt`, in our
current directory. 

#### Pilot model

##### Creating model

We run the following chunk to get the syntax for a model
with the intercepts, all covariates a user-specified uniform prior for the
precision, and 5 chains. The amount of chains is based on the generally
accepted heuristic that 3-5 chains are needed for more complex models:
  
```{r, message=FALSE, warning=FALSE}
model_unif_cov <- template.jags(
  formula = pa ~ (1 | unit) + (1 | hosp) + expe + full + unitsur + we + tech + teach + beds,
  data = RN4CAST_complete,
  file = "JAGSmodel_unif_cov.txt",
  precision.prior = "dunif(0.001, 1000)",
  family = "poisson",
  write.data = FALSE,
  n.chains = 5
)
```


We can now just run the model directly from `R` without needing to
manually copy/paste anything. For this, we use the `run.jags` function
to run the model we just specified our syntax for. We choose a limited
burn-in (4000) and sample (10000) to allow for an initial pilot run.
<!--- Should method=rjags be  used in original model design (see documentation extend.jags - then no need to recompile model when extending) ? --->
  
```{r cache=TRUE}
JAGS.mod_uniform_cov <- run.jags(
  # specify the syntax file
  model_unif_cov,
  # specify the data source (only necessary when write.data = FALSE)
  data = RN4CAST_complete,
  monitor = c(
    "intercept", "unit_precision", "hosp_precision",
    "expe_coefficient", "full_effect", "unitsur_effect",
    "we_coefficient", "tech_effect", "teach_effect", "beds_coefficient",
    "deviance", "dic"
  ),
  burnin = burn_in_iterations, # make informed decision later (choosing speed here)
  sample = sample_iterations, # make informed decision later (choosing speed here)
  method = "rjags"
)
```

##### Visualization

Before assessing the model based through formal measures, we first do a
visual inspection as done before. First we create a convenience function for plotting runjags objects.

```{r}
# Convenience function for plotting
plot_jags <- function(type, var) {
  plot(JAGS.mod_uniform_cov, plot.type = type, vars = var)
}
```

For example, here are the trace plots and autocorrelation plots for the precision of the random
intercept for unit (`unit_precision`) and for hospital (`hosp_precision`), 
as well as the effects for the covariates:
  
```{r, message=FALSE, out.width="50%", fig.align='center'}
plot_jags("trace", "unit_precision")
plot_jags("trace", "hosp_precision")
plot_jags("trace", "expe_coefficient")
plot_jags("trace", "full_effect")
plot_jags("trace", "unitsur_effect")
plot_jags("trace", "we_coefficient")
plot_jags("trace", "tech_effect")
plot_jags("trace", "teach_effect")
plot_jags("trace", "beds_coefficient")

plot_jags("autocorr", "unit_precision")
plot_jags("autocorr", "hosp_precision")
plot_jags("autocorr", "expe_coefficient")
plot_jags("autocorr", "full_effect")
plot_jags("autocorr", "unitsur_effect")
plot_jags("autocorr", "we_coefficient")
plot_jags("autocorr", "tech_effect")
plot_jags("autocorr", "teach_effect")
plot_jags("autocorr", "beds_coefficient")

rmeanplot(JAGS.mod_uniform_cov, parms = "expe_coefficient")
rmeanplot(JAGS.mod_uniform_cov, parms = "full_effect")
rmeanplot(JAGS.mod_uniform_cov, parms = "unitsur_effect")
rmeanplot(JAGS.mod_uniform_cov, parms = "we_coefficient")
rmeanplot(JAGS.mod_uniform_cov, parms = "tech_effect")
rmeanplot(JAGS.mod_uniform_cov, parms = "teach_effect")
rmeanplot(JAGS.mod_uniform_cov, parms = "beds_coefficient")
```

The trace plots seem to be ok. None of the plots get stuck. For the covariates, the trace plots seem to indicate some dependency (i.e. "zigzagging" mean). However, running means plots show that the mean remains stable. Autocorrelation does not seem to be an issue. The autocorrelation generally reaches 0 at lag10 or lag15. The autocorrelation for `unit_precision` reaches 0 at lag25. 


##### Assessment model

The model ran, and we can inspect its results.

```{r poisson-uniform-summary, results = TRUE}
summary(JAGS.mod_uniform_cov)
```

The Gelman-Rubin ANOVA diagnostic (i.e. estimated potential scale reduction factor - PSRF) is close to 1 for all parameters, which is the aim. The MCMC error is small in comparison with the SD, ranging from 0.5 to 1.2.

##### Visualization

Perform visual inspection of trace plots for extended model.
<!--- using reference of previous code block does not seem to work --->
```{r-extend, ref.label=c('traceplots')}
```

```{r, message=FALSE, out.width="50%", fig.align='center'}
plot_jags("trace", "unit_precision")
plot_jags("trace", "hosp_precision")
plot_jags("trace", "expe_coefficient")
plot_jags("trace", "full_effect")
plot_jags("trace", "unitsur_effect")
plot_jags("trace", "we_coefficient")
plot_jags("trace", "tech_effect")
plot_jags("trace", "teach_effect")
plot_jags("trace", "beds_coefficient")

plot_jags("autocorr", "unit_precision")
plot_jags("autocorr", "hosp_precision")
plot_jags("autocorr", "expe_coefficient")
plot_jags("autocorr", "full_effect")
plot_jags("autocorr", "unitsur_effect")
plot_jags("autocorr", "we_coefficient")
plot_jags("autocorr", "tech_effect")
plot_jags("autocorr", "teach_effect")
plot_jags("autocorr", "beds_coefficient")
```
The trace plots look fairly well, with some outliers. None of the trace plots get stuck.

- **Check with DIC whether the inclusion of the covariates improves the fit with respect to the baseline model. Determine what variables to include using this selection criterion.**

We subsequently obtain the DIC (Deviance Information Criterion) to assess the model performance.

```{r}
dic_uniform_cov <- extract.runjags(JAGS.mod_uniform_cov, what = "dic")
dic_uniform_cov
```

The results are the following:
  
-   Mean deviance:  8895 
-   penalty 103.6 
-   Penalized deviance: 8999 

```{r}
# formula <- "pa ~ (1 | unit) + (1 | hosp)"
# level_covariates <- list(c("expe", "full"), c("unitsur", "we"), c("tech", "teach", "beds"))
# used_covariates <- vector(mode = "character")
# better_dic_found <- TRUE
# monitor <- c("k", "intercept", "unit_precision", "hosp_precision")
# method <- "parallel"
# 
# initial_model <- template.jags(
#     formula = formula,
#     data = RN4CAST_complete,
#     file = "JAGSmodel_unif.txt",
#     precision.prior = "dunif(0.001, 1000)",
#     family = "poisson",
#     write.data = FALSE,
#     n.chains = n_chains
# )
# initial_run <- run.jags(
#     initial_model,
#     data = RN4CAST_complete,
#     burnin = burn_in_iterations,
#     sample = sample_iterations,
#     method = method,
#     monitor = monitor
# )
# 
# initial_dic <- extract.runjags(initial_run, what = "dic")
# lowest_cost_so_far <- sum(initial_dic$deviance) + sum(initial_dic$penalty)
# runs <- list()
# dics <- list()
# 
# for (level in seq_len(length(level_covariates))) {
#     unused_covariates <- level_covariates[[level]]
#     while (length(unused_covariates) > 0 && better_dic_found) {
#         better_dic_found <- FALSE
#         costs <- vector(mode = "numeric", length = length(unused_covariates))
#         for (index in seq_len(length(unused_covariates))) {
#             candidate_formula <- as.formula(paste(formula, unused_covariates[index], sep = " + "))
#             print(paste("Now trying", toString(candidate_formula)))
#             candidate_model_file_name <- paste(paste("JAGSmodel", "unif", paste(used_covariates, sep = "", collapse = "_"), unused_covariates[index], sep = "_"), ".txt", sep = "")
#             candidate_model <- template.jags(
#                 formula = candidate_formula,
#                 data = RN4CAST_complete,
#                 file = candidate_model_file_name,
#                 precision.prior = "dunif(0.001, 1000)",
#                 family = "poisson",
#                 write.data = FALSE,
#                 n.chains = n_chains
#             )
#             candidate_run <- run.jags(
#                 candidate_model,
#                 data = RN4CAST_complete,
#                 burnin = burn_in_iterations,
#                 sample = sample_iterations,
#                 method = method,
#                 monitor = monitor
#             )
#             candidate_dic <- extract.runjags(candidate_run, what = "dic")
#             runs <- c(runs, candidate_run)
#             dics <- c(dics, candidate_dic)
#             costs[index] <- sum(candidate_dic$deviance) + sum(candidate_dic$penalty)
#         }
#         lowest_cost_index <- which.min(costs)
#         lowest_cost <- costs[lowest_cost_index]
#         if (lowest_cost < lowest_cost_so_far) {
#             formula <- paste(formula, unused_covariates[lowest_cost_index], sep = " + ")
#             print(paste("New minimal DIC for", toString(formula), "=", lowest_cost))
#             used_covariates <- append(used_covariates, unused_covariates[lowest_cost_index])
#             unused_covariates <- unused_covariates[-c(lowest_cost_index)]
#             lowest_cost_so_far <- lowest_cost
#             better_dic_found <- TRUE
#         }
#     }
#     better_dic_found <- TRUE
# }
```

- **Give all necessary posterior summary measures of the relevant parameters.**

```{r}
JAGS.mod_uniform_cov
```

- **Hint: Standardize all numeric covariates to improve convergence of the MCMC procedure and give initial values to the regression coefficients and parameters related to variances of random effects.**

## Part 2
**In some cases, there is overdispersion compared to what we would be expect in a Poisson model.**

- **Check whether a negative binomial distribution for PA is better than a Poisson distribution.**

## Negative binomial
### Model with covariates: uniform prior for precision

The model is written down in a file, called `JAGSmodel_binom_uniform_cov.txt`, in our
current directory. 

#### Pilot model

##### Creating model

We run the following chunk to get the syntax for a model
with the intercepts, all covariates, user-specified uniform prior prior for the
precision, and 5 chains. The amount of chains is based on the generally
accepted heuristic that 3-5 chains are needed for more complex models:
  
```{r, message=FALSE, warning=FALSE}
model_binom_uniform_cov <- template.jags(
  formula = pa ~ (1 | unit) + (1 | hosp) + expe + full + unitsur + we + tech + teach + beds,
  data = RN4CAST_complete,
  file = "JAGSmodel_binom_uniform_cov.txt",
  precision.prior = "dunif(0.001, 100)",
  family = "negative binomial",
  write.data = FALSE,
  n.chains = 5
)
```


We can now just run the model directly from `R` without needing to
manually copy/paste anything. For this, we use the `run.jags` function
to run the model we just specified our syntax for. We choose a limited
burn-in (4000) and sample (10000) to allow for an initial pilot run.
<!--- Should method=rjags be  used in original model design (see documentation extend.jags - then no need to recompile model when extending) ? --->
  
```{r cache=TRUE}
# JAGS.mod_binom_uniform_cov <- run.jags(
#   # specify the syntax file
#   model_binom_uniform_cov,
#   # specify the data source (only necessary when write.data = FALSE)
#   data = RN4CAST_complete,
#   monitor = c(
#     "k", "intercept", "unit_precision", "hosp_precision",
#     "expe_coefficient", "full_effect", "unitsur_effect",
#     "we_coefficient", "tech_effect", "teach_effect", "beds_coefficient",
#     "deviance"
#   ),
#   burnin = burn_in_iterations, # make informed decision later (choosing speed here)
#   sample = sample_iterations, # make informed decision later (choosing speed here)
#   method = "rjags"
# )
```

##### Visualization

Before assessing the model based through formal measures, we first do a
visual inspection. 

For example, here are the trace plots for the precision of the random
intercept for unit (`unit_precision`) and for hospital (`hosp_precision`), 
as well as for the covariates:
  
```{r, out.width="50%", fig.align='center'}
# plot_jags("trace", "unit_precision")
# plot_jags("trace", "hosp_precision")
# plot_jags("trace", "expe_coefficient")
# plot_jags("trace", "full_effect")
# plot_jags("trace", "unitsur_effect")
# plot_jags("trace", "we_coefficient")
# plot_jags("trace", "tech_effect")
# plot_jags("trace", "teach_effect")
# plot_jags("trace", "beds_coefficient")
# 
# plot_jags("autocorr", "unit_precision")
# plot_jags("autocorr", "hosp_precision")
# plot_jags("autocorr", "expe_coefficient")
# plot_jags("autocorr", "full_effect")
# plot_jags("autocorr", "unitsur_effect")
# plot_jags("autocorr", "we_coefficient")
# plot_jags("autocorr", "tech_effect")
# plot_jags("autocorr", "teach_effect")
# plot_jags("autocorr", "beds_coefficient")
```


<!--- TO BE UPDATED: 
  The trace plot seem fairly ok. There are still quite some outliers, but none of the plots get stuck.
--->
  
  
##### Assessment model
  
The model ran, and we can inspect its results.

```{r negative-binomial-covariates-summary, results = TRUE}
# summary(JAGS.mod_binom_uniform_cov)
```

The Gelman-Rubin ANOVA diagnostic (i.e. estimated potential scale reduction factor - PSRF) is close to 1 for all parameters, which is the aim. Only for  `hosp_precision` it amounts to 1.92. Also, the MCMC error is still fairly large in comparison with the SD, ranging from 0.5 to 5.0. Additional measures (longer chain, accelerating measure) are necessary.


We subsequently obtain the DIC (Deviance Information Criterion) to assess the model performance.
  
The results are the following:
  
```{r}
# dic_binom_uniform_cov <- extract.runjags(JAGS.mod_binom_uniform_cov, what = "dic")
# dic_binom_uniform_cov
```

<!--- TO BE UPDATED:
  The trace plots look fairly well, with some outliers. None of the trace plots get stuck.
--->

- **Motivate your conclusion.**

3. MOtivation,
personally I would. include a general conclusion and motivation, walking through all issues and options we met during
this assignment (modeling time, convergence, parallellism, ....).

Why is the Neg Bino better than poisson => overdispersion, decoupling var and mean, FOCUS ON STATISTICS, 

side note our unknowleddge about the surevey variabiliyt.



[1]: https://bmcnurs.biomedcentral.com/articles/10.1186/1472-6955-10-6 "Sermeus et al. (2011)."


Q2 : CONVERGENCE CHECK
DONE = summary output available
TO DO : SUMMARIZING and we report howto, based on BGR diagnostics, trace plot insights,      and the lag autocorrelation

classical diagnostics : just as in frequence, HIGHLIGHT diag : MCMC error , 

 
summary printout  (in table) + comment it
 
discuss 1. MCMC error (under 5%) + PSRF => BGR diagnostic measurement / effective SS
 

Q3:Rank the hospitals according to their random
this is our rank we tried out + the plot as proposed by Leah??

matrix by Niels => +/- same, 

to look for caterpillarplot and the output object  that Niels got for his matrix

do we know the 'real' impact on PA of the Random Effect hospital ,(considering scaling) 

mention the scaling we did => all effects are around 0 +/- ...

 

Q4 : with Covariates :
Report model, report plots and convergence stats, .... => SO FAR = THE MODELS ARE BUILT WITH ALL COVARIATES
DONE = MODEL WITH ALL COVARS BUILD

TO DO = output of full model and create catterpillar plot + matrix on hospitals  (same as above)
TO DO = Report what we see on (possible) differnces and Why.


 

Q5: DIC FOLLOW UP : model building
DONE : DIC FOR ALL COVARS
TO DO : compare DIC intercept model with DIC intercept + all covars
Personally = THIS IS A QUESTION WHERE WE REPORT OUR INTERPRETATION ABOUT THE DIC parameter (Mean deviance + Penalty added)

BASELINE DIC / FULL MODEL DIC / BEST MODEL DIC 

just discuss slightly our 'logical'  modeling selection approach by levels .
 


Q6: SUMMARY STATS

done , stats available
TO DO : COMMENT OUR summary stats for the full model (MCerror, DIC, ...)


SUMMARY MEASURES + discussion , focus on variable selection with DIC

effective parameters => sum of deviance + eff paramters

 

 

 

Q7 : WHEN OVERDISPERSION:

overdispersion of poisson not suitable anymore, too mean neq var anymore,

solve this with other distribution....NEG BINOMIAL DISTRO
 

1. WHAT DO WE EXPECT : 

look at distro and check which is best.

we do expect : THE TAILS ARE BIGGER IN NEG BIN then POisso, lambda not eq as var...

decouple the mean from the variance with NB


2. Is NEG BIN better => we already have the model

LOOK AT DIC for the Hierarchical fit : half of poisson distro

 

BASELINE DIC / FULL MODEL DIC / BEST MODEL DIC

fun calc : option 3 this is similar with LME4 results => our uninformative prior will .  => this proves that the distribution of data is key., and not the prior choice. 